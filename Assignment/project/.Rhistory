}
dataset_cleaned <- remove_outliers(dataset, numeric_columns)
# Check whether the size of the cleaned data set has been reduced
nrow(dataset_cleaned)
for (col in numeric_columns) {
boxplot(dataset_cleaned[[col]], main = paste("Boxplot of", col, "after cleaning"), ylab = col)
}
ggplot(dataset_cleaned, aes(x = Exam_Score)) +
geom_histogram(binwidth = 5, fill = "blue", color = "black") +
labs(title = "Distribution of Exam Scores", x = "Exam Score", y = "Frequency")
ggplot(dataset_cleaned, aes(x = Hours_Studied, y = Exam_Score)) +
geom_point(color = "blue", alpha = 0.6) +
geom_smooth(method = "lm", se = FALSE, color = "red") +
labs(title = "Hours Studied vs Exam Score with Trend Line", x = "Hours Studied", y = "Exam Score")
ggplot(dataset_cleaned, aes(x = Family_Income, y = Exam_Score)) +
geom_boxplot(fill = "lightcoral") +
labs(title = "Family Income vs Exam Score", x = "Family Income", y = "Exam Score")
ggplot(dataset_cleaned, aes(x = Gender, y = Exam_Score)) +
geom_violin(trim = FALSE, fill = "lightcoral") +
labs(title = "Exam Score Distribution by Gender", x = "Gender", y = "Exam Score")
correlation_matrix <- cor(dataset_cleaned[, sapply(dataset_cleaned, is.numeric)])
ggcorrplot(correlation_matrix, lab = TRUE)
dataset_classification <- dataset_cleaned
# Convert Exam_Score to a binary categorical variable (threshold 67), Pass = 1 Fail =- 0
dataset_classification$Pass_Fail <- ifelse(dataset_classification$Exam_Score >= 67, 1, 0)
# Delete the Exam_Score column because we have generated binary categorical variables
dataset_classification <- dataset_classification %>% select(-Exam_Score)
# Check the distribution of categories
table(dataset_classification$Pass_Fail)
dataset_classification <- dataset_classification %>%
select(-Previous_Scores)
dataset_classification <- dataset_classification %>%
select(-Gender)
dataset_classification_dummy <- dataset_classification
target <- dataset_classification$Pass_Fail
# one-hot encoding of character variables, excluding target variables
dataset_classification_dummy <- dataset_classification_dummy %>%
select(-Pass_Fail) %>%
mutate_at(vars(Parental_Involvement, Access_to_Resources, Extracurricular_Activities,
Motivation_Level, Internet_Access, Family_Income, Teacher_Quality,
School_Type, Peer_Influence, Learning_Disabilities,
Parental_Education_Level, Distance_from_Home),
funs(as.factor(.)))
# one-hot code the remaining features using dummyVars
dummies <- dummyVars(~., data = dataset_classification)
dataset_classification_dummy$Pass_Fail <- target
dataset_classification_dummy <- data.frame(predict(dummies, newdata = dataset_classification_dummy))
# Calculate the correlation matrix of the numerical variables
Vars <- setdiff(colnames(dataset_classification_dummy), "Pass_Fail")
correlation_matrix <- cor(dataset_classification_dummy$Pass_Fail, dataset_classification_dummy[, Vars], use = 'pairwise.complete.obs')
sorted_corr <- correlation_matrix[, order(abs(correlation_matrix), decreasing = TRUE)]
# Remove highly correlated features (assuming a correlation coefficient threshold of 0.8)
high_corr_vars <- names(sorted_corr[abs(sorted_corr) > 0.8])
dataset_classification_dummy <- dataset_classification_dummy %>%
select(-all_of(high_corr_vars))
set.seed(24121014)
trainIndex <- createDataPartition(dataset_classification_dummy$Pass_Fail, p = 0.8,
list = FALSE,
times = 1)
train_data <- dataset_classification_dummy[trainIndex,]
test_data <- dataset_classification_dummy[-trainIndex,]
dim(train_data)
dim(test_data)
# Get all dummy variable column names (excluding Pass_Fail column)
dummyVars <- setdiff(colnames(dataset_classification_dummy), "Pass_Fail")
# Define the AUC evaluation function
calcAUC <- function(predcol, outcol) {
perf <- performance(prediction(as.numeric(predcol), outcol == 1), "auc")
as.numeric(perf@y.values)
}
# Make predictions for each single variable and calculate the AUC
for (v in dummyVars) {
train_pred <- train_data[[v]]
test_pred <- test_data[[v]]
# Calculate the AUC of the training set and the test set
aucTrain <- calcAUC(train_pred, train_data$Pass_Fail)
if (aucTrain >= 0.53) {
aucTest <- calcAUC(test_pred, test_data$Pass_Fail)
print(sprintf("%s: trainAUC: %4.3f; testAUC: %4.3f", v, aucTrain, aucTest))
}
}
selected_trainvars <- train_data[, c('Hours_Studied','Attendance','Parental_InvolvementHigh','Access_to_ResourcesHigh','Tutoring_Sessions', 'Peer_InfluencePositive', 'Parental_Education_LevelPostgraduate', 'Distance_from_HomeNear','Pass_Fail')]
selected_testvars <- test_data[, c('Hours_Studied','Attendance','Parental_InvolvementHigh','Access_to_ResourcesHigh','Tutoring_Sessions', 'Peer_InfluencePositive', 'Parental_Education_LevelPostgraduate', 'Distance_from_HomeNear','Pass_Fail')]
train_data$Pass_Fail <- as.factor(train_data$Pass_Fail)
test_data$Pass_Fail <- as.factor(test_data$Pass_Fail)
tree_model <- rpart(Pass_Fail ~ ., data=train_data, method="class")
# Prediction for the test data
tree_pred_class <- predict(tree_model, newdata=test_data, type="class")
# Evaluate the performance (confusion matrix)
conf_matrix <- confusionMatrix(tree_pred_class, test_data$Pass_Fail)
print(conf_matrix)
nb_model <- naiveBayes(Pass_Fail ~ ., data=train_data)
# Prediction for the test data
nb_pred_class <- predict(nb_model, newdata=test_data)
# Evaluate the performance (confusion matrix)
conf_matrix_nb <- confusionMatrix(nb_pred_class, test_data$Pass_Fail)
print(conf_matrix_nb)
# Decision Tree Prediction probabilities (use type="prob" for predicted probabilities)
tree_pred_prob <- predict(tree_model, newdata=test_data, type="prob")
# Naive Bayes Prediction probabilities
nb_pred_prob <- predict(nb_model, newdata=test_data, type="raw")
# Plot ROC curve for Decision Tree
roc_tree <- roc(test_data$Pass_Fail, tree_pred_prob[,2])
# Plot ROC curve for Naive Bayes
roc_nb <- roc(test_data$Pass_Fail, nb_pred_prob[,2])
auc(roc_nb)
auc(roc_tree)
plot(roc_tree, col = "blue", lwd = 2, main = "ROC Curve for Decision Tree")
plot(roc_nb, col = "red", lwd = 2, add=TRUE)
legend("bottomright", legend=c("Decision Tree", "Naive Bayes"), col=c("blue", "red"), lwd=2)
selected_trainvars$Pass_Fail <- as.factor(selected_trainvars$Pass_Fail)
selected_testvars$Pass_Fail <- as.factor(selected_testvars$Pass_Fail)
tree_model <- rpart(Pass_Fail ~ ., data=selected_trainvars, method="class")
# Prediction for the test data
tree_pred_class <- predict(tree_model, newdata=selected_testvars, type="class")
# Evaluate the performance (confusion matrix)
conf_matrix <- confusionMatrix(tree_pred_class, selected_testvars$Pass_Fail)
print(conf_matrix)
nb_model <- naiveBayes(Pass_Fail ~ ., data=selected_trainvars)
# Prediction for the test data
nb_pred_class <- predict(nb_model, newdata=selected_testvars)
# Evaluate the performance (confusion matrix)
conf_matrix_nb <- confusionMatrix(nb_pred_class, selected_testvars$Pass_Fail)
print(conf_matrix_nb)
# Decision Tree Prediction probabilities (use type="prob" for predicted probabilities)
tree_pred_prob <- predict(tree_model, newdata=selected_testvars, type="prob")
# Naive Bayes Prediction probabilities
nb_pred_prob <- predict(nb_model, newdata=selected_testvars, type="raw")
# Plot ROC curve for Decision Tree
roc_tree <- roc(selected_testvars$Pass_Fail, tree_pred_prob[,2])
# Plot ROC curve for Naive Bayes
roc_nb <- roc(selected_testvars$Pass_Fail, nb_pred_prob[,2])
auc(roc_nb)
auc(roc_tree)
plot(roc_tree, col = "blue", lwd = 2, main = "ROC Curve for Decision Tree")
plot(roc_nb, col = "red", lwd = 2, add=TRUE)
legend("bottomright", legend=c("Decision Tree", "Naive Bayes"), col=c("blue", "red"), lwd=2)
# Define a custom function for the Decision Tree model
assign("model_type.lime_model_rpart", function(object, ...) {
return("classification")
}, envir = .GlobalEnv)
lime_model_rpart <- function(x) {
class(x) <- c("lime_model_rpart", class(x))
x
}
# Define a custom predict function for Naive Bayes model
predict_proba <- function(model, newdata) {
predict(model, newdata, type = "raw")
}
# Define model type and prediction functions for Naive Bayes
assign("model_type.nb", function(x, ...) {
return(x$type)
}, envir = .GlobalEnv)
assign("predict_model.nb", function(x, newdata, ...) {
predict_result <- predict(x$model, newdata = newdata, type = "raw")
return(data.frame(predict_result))
}, envir = .GlobalEnv)
nb_model_proba <- list(model = nb_model, predict = predict_proba, type = "classification")
class(nb_model_proba) <- 'nb'
# Prepare the explainer
explainer_tree <- lime(selected_trainvars, lime_model_rpart(tree_model))
explainer_nb <- lime(selected_trainvars, nb_model_proba)
# Generate explanations
explanation_tree <- explain(selected_testvars[1:3, ], explainer_tree, n_labels = 1, n_features = 3)
explanation_nb <- explain(selected_testvars[1:3, ], explainer_nb, n_labels = 1, n_features = 3)
print(explanation_tree)
print(explanation_nb)
plot_features(explanation_tree)
plot_features(explanation_nb)
selected_features <- dataset_cleaned%>%
select(Hours_Studied, Attendance, Tutoring_Sessions, Exam_Score)
# Standardized numerical features
numeric_features <- selected_features[, c("Hours_Studied", "Attendance", "Tutoring_Sessions","Exam_Score")]
scaled_data <- scale(numeric_features)
pca_model <- prcomp(scaled_data, center = TRUE, scale. = TRUE)
summary(pca_model)
# The first few principal components are selected, so that the cumulative variance interpretation rate reaches 90%
pca_data <- data.frame(pca_model$x[, 1:which(cumsum(pca_model$sdev^2 / sum(pca_model$sdev^2)) >= 0.9)[1]])
wss <- function(data, k) {
kmeans(data, k, nstart = 10)$tot.withinss
}
# Try different K values and calculate WSS
k_values <- 1:10
wss_values <- map_dbl(k_values, wss, data = pca_data)
plot(k_values, wss_values, type = "b", pch = 19, frame = FALSE,
xlab = "Number of K",
ylab = "Total Within-Cluster Sum of Squares (WSS)",
main = "Elbow Method for Optimal K")
silhouette_scores <- numeric(length(k_values))
# Do k-means clustering for each K value and calculate the Silhouette Score
for (i in 1:length(k_values)) {
k <- k_values[i]
kmeans_model <- kmeans(pca_data, centers = k, nstart = 25)
dist_matrix <- dist(pca_data)
silhouette_result <- silhouette(kmeans_model$cluster, dist_matrix)
if (length(dim(silhouette_result)) == 2) {
silhouette_scores[i] <- mean(silhouette_result[, 3])  # Silhouette宽度在结果的第3列
} else {
silhouette_scores[i] <- NA  # 如果结果不对，则填充NA
}
}
# Draw the Silhouette Score for each K value
plot(k_values, silhouette_scores, type = "b",
xlab = "Number of clusters (K)", ylab = "Silhouette Score",
main = "Silhouette Score for Optimal K")
k = 2
kmeans_result <- kmeans(pca_data, centers = k, nstart = 10)
# Visualize the clustering results
pca_data$cluster <- as.factor(kmeans_result$cluster)
ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster)) +
geom_point(size = 3) +
labs(title = paste("K-means Clustering (K=", k, ") on PCA-reduced Data", sep=""),
x = "Principal Component 1", y = "Principal Component 2") +
theme_minimal()
# Combine raw data with clustering results
clustered_data <- cbind(dataset_cleaned , cluster = kmeans_result$cluster)
# The average value of each feature is calculated according to the clustering results
cluster_summary <- clustered_data %>%
group_by(cluster) %>%
summarise(across(c(Hours_Studied, Attendance, Tutoring_Sessions), mean, na.rm = TRUE))
# Print the average of each cluster
print(cluster_summary)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(dplyr)
library(ggplot2)
library(ggcorrplot)
library(caret)
library(ROCR)
library(e1071)
library(pROC)
library(rpart)
library(lime)
library(cluster)
library(purrr)
# replace empty space with NA
dataset <- read.csv("StudentPerformanceFactors.csv", na.strings = c("", "NA"))
str(dataset)
summary(dataset$Exam_Score)
sum(dataset$Exam_Score == 101)
missing_data <- colSums(is.na(dataset))
print(missing_data)
# Teacher_Quality distribution
table(dataset$Teacher_Quality, useNA = "ifany")
# Distance_from_Home distribution
table(dataset$Distance_from_Home, useNA = "ifany")
# Parental_Education_Level distribution
table(dataset$Parental_Education_Level, useNA = "ifany")
# View the distribution of Family_Income and Parental_Education_Level
table(dataset$Family_Income, dataset$Parental_Education_Level, useNA = "ifany")
# For Parental_Education_Level
dataset <- dataset %>%
group_by(Family_Income) %>%
mutate(Parental_Education_Level = ifelse(is.na(Parental_Education_Level),
sample(Parental_Education_Level[!is.na(Parental_Education_Level)],
sum(is.na(Parental_Education_Level)), replace = TRUE),
Parental_Education_Level))
# For Teacher_Quality
dataset$Teacher_Quality[is.na(dataset$Teacher_Quality)] <- mode(dataset$Teacher_Quality)
#For Distance_from_Home
dataset$Distance_from_Home[is.na(dataset$Distance_from_Home)] <- mode(dataset$Distance_from_Home)
# Check for missing values
sum(is.na(dataset$Parental_Education_Level))
sum(is.na(dataset$Teacher_Quality))
sum(is.na(dataset$Distance_from_Home))
# Visualize outliers for numeric columns
numeric_columns <- c("Hours_Studied", "Attendance", "Sleep_Hours",
"Previous_Scores", "Tutoring_Sessions", "Physical_Activity")
par(mfrow = c(2, 3),  # The layout is 2 rows and 4 columns
mar = c(4, 4, 2, 1),  # Adjust margins
cex.main = 1.5,  # Increase font size
cex.lab = 1.2)
# Create boxplot
for (col in numeric_columns) {
boxplot(dataset[[col]], main = paste("Boxplot of", col), ylab = col)
}
numeric_columns <- c("Hours_Studied", "Tutoring_Sessions")
remove_outliers <- function(data, cols) {
for (col in cols) {
Q1 <- quantile(data[[col]], 0.25, na.rm = TRUE)
Q3 <- quantile(data[[col]], 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
# Filter outliers that exceed the lower and upper limits
data <- data %>%
filter(!!sym(col) >= lower_bound & !!sym(col) <= upper_bound)
}
return(data)
}
dataset_cleaned <- remove_outliers(dataset, numeric_columns)
# Check whether the size of the cleaned data set has been reduced
nrow(dataset_cleaned)
for (col in numeric_columns) {
boxplot(dataset_cleaned[[col]], main = paste("Boxplot of", col, "after cleaning"), ylab = col)
}
ggplot(dataset_cleaned, aes(x = Exam_Score)) +
geom_histogram(binwidth = 5, fill = "blue", color = "black") +
labs(title = "Distribution of Exam Scores", x = "Exam Score", y = "Frequency")
ggplot(dataset_cleaned, aes(x = Hours_Studied, y = Exam_Score)) +
geom_point(color = "blue", alpha = 0.6) +
geom_smooth(method = "lm", se = FALSE, color = "red") +
labs(title = "Hours Studied vs Exam Score with Trend Line", x = "Hours Studied", y = "Exam Score")
ggplot(dataset_cleaned, aes(x = Family_Income, y = Exam_Score)) +
geom_boxplot(fill = "lightcoral") +
labs(title = "Family Income vs Exam Score", x = "Family Income", y = "Exam Score")
ggplot(dataset_cleaned, aes(x = Gender, y = Exam_Score)) +
geom_violin(trim = FALSE, fill = "lightcoral") +
labs(title = "Exam Score Distribution by Gender", x = "Gender", y = "Exam Score")
correlation_matrix <- cor(dataset_cleaned[, sapply(dataset_cleaned, is.numeric)])
ggcorrplot(correlation_matrix, lab = TRUE)
dataset_classification <- dataset_cleaned
# Convert Exam_Score to a binary categorical variable (threshold 67), Pass = 1 Fail =- 0
dataset_classification$Pass_Fail <- ifelse(dataset_classification$Exam_Score >= 67, 1, 0)
# Delete the Exam_Score column because we have generated binary categorical variables
dataset_classification <- dataset_classification %>% select(-Exam_Score)
# Check the distribution of categories
table(dataset_classification$Pass_Fail)
dataset_classification <- dataset_classification %>%
select(-Previous_Scores)
dataset_classification <- dataset_classification %>%
select(-Gender)
dataset_classification_dummy <- dataset_classification
target <- dataset_classification$Pass_Fail
# one-hot encoding of character variables, excluding target variables
dataset_classification_dummy <- dataset_classification_dummy %>%
select(-Pass_Fail) %>%
mutate_at(vars(Parental_Involvement, Access_to_Resources, Extracurricular_Activities,
Motivation_Level, Internet_Access, Family_Income, Teacher_Quality,
School_Type, Peer_Influence, Learning_Disabilities,
Parental_Education_Level, Distance_from_Home),
funs(as.factor(.)))
# one-hot code the remaining features using dummyVars
dummies <- dummyVars(~., data = dataset_classification)
dataset_classification_dummy$Pass_Fail <- target
dataset_classification_dummy <- data.frame(predict(dummies, newdata = dataset_classification_dummy))
# Calculate the correlation matrix of the numerical variables
Vars <- setdiff(colnames(dataset_classification_dummy), "Pass_Fail")
correlation_matrix <- cor(dataset_classification_dummy$Pass_Fail, dataset_classification_dummy[, Vars], use = 'pairwise.complete.obs')
sorted_corr <- correlation_matrix[, order(abs(correlation_matrix), decreasing = TRUE)]
# Remove highly correlated features (assuming a correlation coefficient threshold of 0.8)
high_corr_vars <- names(sorted_corr[abs(sorted_corr) > 0.8])
dataset_classification_dummy <- dataset_classification_dummy %>%
select(-all_of(high_corr_vars))
set.seed(24121014)
trainIndex <- createDataPartition(dataset_classification_dummy$Pass_Fail, p = 0.8,
list = FALSE,
times = 1)
train_data <- dataset_classification_dummy[trainIndex,]
test_data <- dataset_classification_dummy[-trainIndex,]
dim(train_data)
dim(test_data)
# Get all dummy variable column names (excluding Pass_Fail column)
dummyVars <- setdiff(colnames(dataset_classification_dummy), "Pass_Fail")
# Define the AUC evaluation function
calcAUC <- function(predcol, outcol) {
perf <- performance(prediction(as.numeric(predcol), outcol == 1), "auc")
as.numeric(perf@y.values)
}
# Make predictions for each single variable and calculate the AUC
for (v in dummyVars) {
train_pred <- train_data[[v]]
test_pred <- test_data[[v]]
# Calculate the AUC of the training set and the test set
aucTrain <- calcAUC(train_pred, train_data$Pass_Fail)
if (aucTrain >= 0.53) {
aucTest <- calcAUC(test_pred, test_data$Pass_Fail)
print(sprintf("%s: trainAUC: %4.3f; testAUC: %4.3f", v, aucTrain, aucTest))
}
}
selected_trainvars <- train_data[, c('Hours_Studied','Attendance','Parental_InvolvementHigh','Access_to_ResourcesHigh','Tutoring_Sessions', 'Peer_InfluencePositive', 'Parental_Education_LevelPostgraduate', 'Distance_from_HomeNear','Pass_Fail')]
selected_testvars <- test_data[, c('Hours_Studied','Attendance','Parental_InvolvementHigh','Access_to_ResourcesHigh','Tutoring_Sessions', 'Peer_InfluencePositive', 'Parental_Education_LevelPostgraduate', 'Distance_from_HomeNear','Pass_Fail')]
train_data$Pass_Fail <- as.factor(train_data$Pass_Fail)
test_data$Pass_Fail <- as.factor(test_data$Pass_Fail)
tree_model <- rpart(Pass_Fail ~ ., data=train_data, method="class")
# Prediction for the test data
tree_pred_class <- predict(tree_model, newdata=test_data, type="class")
# Evaluate the performance (confusion matrix)
conf_matrix <- confusionMatrix(tree_pred_class, test_data$Pass_Fail)
print(conf_matrix)
nb_model <- naiveBayes(Pass_Fail ~ ., data=train_data)
# Prediction for the test data
nb_pred_class <- predict(nb_model, newdata=test_data)
# Evaluate the performance (confusion matrix)
conf_matrix_nb <- confusionMatrix(nb_pred_class, test_data$Pass_Fail)
print(conf_matrix_nb)
# Decision Tree Prediction probabilities (use type="prob" for predicted probabilities)
tree_pred_prob <- predict(tree_model, newdata=test_data, type="prob")
# Naive Bayes Prediction probabilities
nb_pred_prob <- predict(nb_model, newdata=test_data, type="raw")
# Plot ROC curve for Decision Tree
roc_tree <- roc(test_data$Pass_Fail, tree_pred_prob[,2])
# Plot ROC curve for Naive Bayes
roc_nb <- roc(test_data$Pass_Fail, nb_pred_prob[,2])
auc(roc_nb)
auc(roc_tree)
plot(roc_tree, col = "blue", lwd = 2, main = "ROC Curve for Decision Tree")
plot(roc_nb, col = "red", lwd = 2, add=TRUE)
legend("bottomright", legend=c("Decision Tree", "Naive Bayes"), col=c("blue", "red"), lwd=2)
selected_trainvars$Pass_Fail <- as.factor(selected_trainvars$Pass_Fail)
selected_testvars$Pass_Fail <- as.factor(selected_testvars$Pass_Fail)
tree_model <- rpart(Pass_Fail ~ ., data=selected_trainvars, method="class")
# Prediction for the test data
tree_pred_class <- predict(tree_model, newdata=selected_testvars, type="class")
# Evaluate the performance (confusion matrix)
conf_matrix <- confusionMatrix(tree_pred_class, selected_testvars$Pass_Fail)
print(conf_matrix)
nb_model <- naiveBayes(Pass_Fail ~ ., data=selected_trainvars)
# Prediction for the test data
nb_pred_class <- predict(nb_model, newdata=selected_testvars)
# Evaluate the performance (confusion matrix)
conf_matrix_nb <- confusionMatrix(nb_pred_class, selected_testvars$Pass_Fail)
print(conf_matrix_nb)
# Decision Tree Prediction probabilities (use type="prob" for predicted probabilities)
tree_pred_prob <- predict(tree_model, newdata=selected_testvars, type="prob")
# Naive Bayes Prediction probabilities
nb_pred_prob <- predict(nb_model, newdata=selected_testvars, type="raw")
# Plot ROC curve for Decision Tree
roc_tree <- roc(selected_testvars$Pass_Fail, tree_pred_prob[,2])
# Plot ROC curve for Naive Bayes
roc_nb <- roc(selected_testvars$Pass_Fail, nb_pred_prob[,2])
auc(roc_nb)
auc(roc_tree)
plot(roc_tree, col = "blue", lwd = 2, main = "ROC Curve for Decision Tree")
plot(roc_nb, col = "red", lwd = 2, add=TRUE)
legend("bottomright", legend=c("Decision Tree", "Naive Bayes"), col=c("blue", "red"), lwd=2)
# Define a custom function for the Decision Tree model
assign("model_type.lime_model_rpart", function(object, ...) {
return("classification")
}, envir = .GlobalEnv)
lime_model_rpart <- function(x) {
class(x) <- c("lime_model_rpart", class(x))
x
}
# Define a custom predict function for Naive Bayes model
predict_proba <- function(model, newdata) {
predict(model, newdata, type = "raw")
}
# Define model type and prediction functions for Naive Bayes
assign("model_type.nb", function(x, ...) {
return(x$type)
}, envir = .GlobalEnv)
assign("predict_model.nb", function(x, newdata, ...) {
predict_result <- predict(x$model, newdata = newdata, type = "raw")
return(data.frame(predict_result))
}, envir = .GlobalEnv)
nb_model_proba <- list(model = nb_model, predict = predict_proba, type = "classification")
class(nb_model_proba) <- 'nb'
# Prepare the explainer
explainer_tree <- lime(selected_trainvars, lime_model_rpart(tree_model))
explainer_nb <- lime(selected_trainvars, nb_model_proba)
# Generate explanations
explanation_tree <- explain(selected_testvars[1:3, ], explainer_tree, n_labels = 1, n_features = 3)
explanation_nb <- explain(selected_testvars[1:3, ], explainer_nb, n_labels = 1, n_features = 3)
print(explanation_tree)
print(explanation_nb)
plot_features(explanation_tree)
plot_features(explanation_nb)
selected_features <- dataset_cleaned%>%
select(Hours_Studied, Attendance, Tutoring_Sessions, Exam_Score)
# Standardized numerical features
numeric_features <- selected_features[, c("Hours_Studied", "Attendance", "Tutoring_Sessions","Exam_Score")]
scaled_data <- scale(numeric_features)
pca_model <- prcomp(scaled_data, center = TRUE, scale. = TRUE)
summary(pca_model)
# The first few principal components are selected, so that the cumulative variance interpretation rate reaches 90%
pca_data <- data.frame(pca_model$x[, 1:which(cumsum(pca_model$sdev^2 / sum(pca_model$sdev^2)) >= 0.9)[1]])
wss <- function(data, k) {
kmeans(data, k, nstart = 10)$tot.withinss
}
# Try different K values and calculate WSS
k_values <- 1:10
wss_values <- map_dbl(k_values, wss, data = pca_data)
plot(k_values, wss_values, type = "b", pch = 19, frame = FALSE,
xlab = "Number of K",
ylab = "Total Within-Cluster Sum of Squares (WSS)",
main = "Elbow Method for Optimal K")
silhouette_scores <- numeric(length(k_values))
# Do k-means clustering for each K value and calculate the Silhouette Score
for (i in 1:length(k_values)) {
k <- k_values[i]
kmeans_model <- kmeans(pca_data, centers = k, nstart = 25)
dist_matrix <- dist(pca_data)
silhouette_result <- silhouette(kmeans_model$cluster, dist_matrix)
if (length(dim(silhouette_result)) == 2) {
silhouette_scores[i] <- mean(silhouette_result[, 3])  # Silhouette宽度在结果的第3列
} else {
silhouette_scores[i] <- NA  # 如果结果不对，则填充NA
}
}
# Draw the Silhouette Score for each K value
plot(k_values, silhouette_scores, type = "b",
xlab = "Number of clusters (K)", ylab = "Silhouette Score",
main = "Silhouette Score for Optimal K")
k = 2
kmeans_result <- kmeans(pca_data, centers = k, nstart = 10)
# Visualize the clustering results
pca_data$cluster <- as.factor(kmeans_result$cluster)
ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster)) +
geom_point(size = 3) +
labs(title = paste("K-means Clustering (K=", k, ") on PCA-reduced Data", sep=""),
x = "Principal Component 1", y = "Principal Component 2") +
theme_minimal()
# Combine raw data with clustering results
clustered_data <- cbind(dataset_cleaned , cluster = kmeans_result$cluster)
# The average value of each feature is calculated according to the clustering results
cluster_summary <- clustered_data %>%
group_by(cluster) %>%
summarise(across(c(Hours_Studied, Attendance, Tutoring_Sessions), mean, na.rm = TRUE))
# Print the average of each cluster
print(cluster_summary)
library(shiny); runApp('D:/Documents/Collection/SEM-2/CITS4009/Assignment/project/shinny.R')
runApp('D:/Documents/Collection/SEM-2/CITS4009/Assignment/project/shinny.R')
